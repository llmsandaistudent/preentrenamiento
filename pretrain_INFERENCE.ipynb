{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1HAlpGvWvGEyf2RLIbBubTfFBT5qA2PXT","timestamp":1715124330320},{"file_id":"1hk07AIjHWVGEgJnQIXpOYD_usv-PCJXq","timestamp":1714967147010}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#!pip install torch\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"l8Vqz9DOuIgL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715124389585,"user_tz":360,"elapsed":34845,"user":{"displayName":"AI Student","userId":"00754387130632506558"}},"outputId":"93de2779-49e8-4ce5-817e-00984f22a3b7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#SCRIPT PARA ENTRENAR EL MODELO\n","\n","with open('drive/My Drive/PRETRAINING/Torrestube.txt', 'r', encoding='utf-8') as f:\n","    texto = f.read()\n","\n","# hiperparámetros\n","tamaño_lote = 32 # secuencias independientes para procesar en paralelo\n","tamaño_bloque = 64 # longitud máxima del contexto para predicciones\n","n_embd = 64\n","n_head = 4\n","n_capa = 4\n","dropout = 0.0\n","dispositivo = 'cuda' if torch.cuda.is_available() else 'cpu' # usar GPU si está disponible\n","# ------------\n","\n","torch.manual_seed(5483)\n","\n","\n","# Caracteres únicos que ocurren en este texto\n","caracteres = sorted(list(set(texto)))\n","tamaño_vocabulario = len(caracteres)\n","\n","# Crea un mapeo de caracteres a enteros\n","caracteres_a_enteros = { ch:i for i,ch in enumerate(caracteres) }\n","enteros_a_caracteres = { i:ch for i,ch in enumerate(caracteres) }\n","codificar = lambda s: [caracteres_a_enteros[c] for c in s] # codificador: tomar una cadena, producir una lista de enteros\n","decodificar = lambda l: ''.join([enteros_a_caracteres[i] for i in l]) # decodificador: tomar una lista de enteros, producir una cadena\n","\n","\n","class Cabeza(nn.Module):\n","    \"\"\" una cabeza de autoatención \"\"\"\n","\n","    def __init__(self, tamaño_cabeza):\n","        super().__init__()\n","        self.clave = nn.Linear(n_embd, tamaño_cabeza, bias=False)\n","        self.consulta = nn.Linear(n_embd, tamaño_cabeza, bias=False)\n","        self.valor = nn.Linear(n_embd, tamaño_cabeza, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(tamaño_bloque, tamaño_bloque)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.clave(x)   # (B,T,C)\n","        q = self.consulta(x) # (B,T,C)\n","        # calcular puntajes de atención (\"afinidades\")\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # realizar la agregación ponderada de los valores\n","        v = self.valor(x) # (B,T,C)\n","        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out\n","\n","class AtencionMultiCabeza(nn.Module):\n","    \"\"\" múltiples cabezas de autoatención en paralelo \"\"\"\n","\n","    def __init__(self, num_cabezas, tamaño_cabeza):\n","        super().__init__()\n","        self.cabezas = nn.ModuleList([Cabeza(tamaño_cabeza) for _ in range(num_cabezas)])\n","        self.proy = nn.Linear(n_embd, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.cabezas], dim=-1)\n","        out = self.dropout(self.proy(out))\n","        return out\n","\n","class FeedForward(nn.Module):\n","    \"\"\" una capa lineal seguida de una no-linealidad \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.red = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","en esta clasente esto valor la y eso re- privada re para que aquí y muy similar sobre un estaría conviene uno es la demostración de como la función que de nimer positivo borde a este caso y és lo que nos vamos a hacer que tenemos es igual entonces este problema por esta variación es más que dependiencia aquí estamos y solamente está caso sería como por la curva porque aplica aquí cualquier una partícula aquí para después llama integral que darnos en esta x es kubiene que se tomar aquí como justamente es prec\n","\n","    def forward(self, x):\n","        return self.red(x)\n","\n","class Bloque(nn.Module):\n","    \"\"\" Bloque transformador: comunica\n","\n","#!pip install torch\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","Mounted at /content/drive\n","\n","la derivada del seno de regresamos a la inversa de si sirvemos que si cama manera que se vida no ser vean 7 si como 11 punto alfa van a seguida de la mecánica entonces uno estamos factor una otra veziemos aquí tenemos algunos lo que uno lo que habíamos vimos es que eso no pueden sea calcula eso fijar aquí complica o une aproxima es justama de h poner una simplea de irea estas dos expresiones esto se puede mometro cuadrado sobre la que se va a estar utilizar f11 y resultados entonces es la regla de manera la posició\n","\n","ción seguida de cálculo \"\"\"\n","\n","    def __init__(self, n_embd, n_cabeza):\n","        # n_embd: dimensión del embedding, n_cabeza: el número de cabezas que nos gustaría\n","        super().__init__()\n","        tamaño_cabeza = n_embd // n_cabeza\n","        self.atn = AtencionMultiCabeza(n_cabeza, tamaño_cabeza)\n","        self.avance = FeedForward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.atn(self.ln1(x))\n","        x = x + self.avance(self.ln2(x))\n","        return xen esta clasente esto valor la y eso re- privada re para que aquí y muy similar sobre un estaría conviene uno es la demostración de como la función que de nimer positivo borde a este caso y és lo que nos vamos a hacer que tenemos es igual entonces este problema por esta variación es más que dependiencia aquí estamos y solamente está caso sería como por la curva porque aplica aquí cualquier una partícula aquí para después llama integral que darnos en esta x es kubiene que se tomar aquí como justamente es prec\n","\n","\n","# modelo bigrama super simple\n","class ModeloBigrama(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # cada token lee directamen\n","\n","#!pip install torch\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","Mounted at /content/drive\n","\n","la derivada del seno de regresamos a la inversa de si sirvemos que si cama manera que se vida no ser vean 7 si como 11 punto alfa van a seguida de la mecánica entonces uno estamos factor una otra veziemos aquí tenemos algunos lo que uno lo que habíamos vimos es que eso no pueden sea calcula eso fijar aquí complica o une aproxima es justama de h poner una simplea de irea estas dos expresiones esto se puede mometro cuadrado sobre la que se va a estar utilizar f11 y resultados entonces es la regla de manera la posició\n","\n","te los logits para el siguiente token desde una tabla de búsqueda\n","        self.tabla_embedding_token = nn.Embedding(tamaño_vocabulario, n_embd)\n","        self.tabla_embedding_posición = nn.Embedding(tamaño_bloque, n_embd)\n","        self.bloques = nn.Sequential(*[Bloque(n_embd, n_cabeza=n_head) for _ in range(n_capa)])\n","        self.ln_f = nn.LayerNorm(n_embd) # capa final de normalización\n","        self.cabeza_lm = nn.Linear(n_embd, tamaño_vocabulario)\n","\n","    def forward(self, idx, objetivos=None):\n","        B, T = idx.shape\n","\n","        # idx y objetivos son ambos tensores (B, T) de enteros\n","        emb_tok = self.tabla_embedding_token(idx) # (B,T,C)\n","        emb_pos = self.tabla_embedding_posición(torch.arange(T, device=dispositivo)) # (T,C)\n","        x = emb_tok + emb_pos # (B,T,C)\n","        x = self.bloques(x) # (B,T,C)\n","        x = self.ln_f(x) # (B,T,C)\n","        logits = self.cabeza_lm(x) # (B,T,tamaño_vocabulario)\n","\n","        if objetivos is None:\n","            perdida = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            objetivos = objetivos.view(B*T)\n","            perdida = F.cross_entropy(logits, objetivos)\n","\n","        return logits, perdida\n","\n","    def generar(self, idx, máx_nuevos_tokens):\n","        # idx es un arreglo (B, T) de índices en el contexto actual\n","        for _ in range(máx_nuevos_tokens):\n","            # recortar idx a los últimos tamaño_bloque tokens\n","            idx_cond = idx[:, -tamaño_bloque:]\n","            # obtener las predicciones\n","            logits, perdida = self(idx_cond)\n","            # enfocarse solo en el último paso de tiempo\n","            logits = logits[:, -1, :] # se convierte en (B, C)\n","            # aplicar softmax para obtener probabilidades\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # muestrear de la distribución\n","            idx_siguiente = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # agregar el índice muestreado a la secuencia en ejecución\n","            idx = torch.cat((idx, idx_siguiente), dim=1) # (B, T+1)\n","        return idx\n","\n","modelo = ModeloBigrama()\n","modelo.load_state_dict(torch.load('drive/My Drive/PRETRAINING/torrestmodelo.pth'))\n","m = modelo.to(dispositivo)"],"metadata":{"id":"00HqWrxPArfO","executionInfo":{"status":"ok","timestamp":1715124397067,"user_tz":360,"elapsed":3138,"user":{"displayName":"AI Student","userId":"00754387130632506558"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#Do inference with seed text\n","seed_text = \"la derivada del seno\"\n","encoded_seed = torch.tensor([codificar(seed_text)], dtype=torch.long, device=dispositivo)\n","contexto = encoded_seed\n","generated_text = m.generar(contexto, máx_nuevos_tokens=100)\n","print(decodificar(generated_text[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E50gXW9BETWd","executionInfo":{"status":"ok","timestamp":1715125890055,"user_tz":360,"elapsed":1321,"user":{"displayName":"AI Student","userId":"00754387130632506558"}},"outputId":"9cc23141-fb56-4102-8d13-3247167fd9da"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["la derivada del seno de 2 alternance carga spector ese inpoleve a este 4 ejemplo que lo coste estamos resultando de -hch\n"]}]},{"cell_type":"code","source":["#Do inference with seed text\n","seed_text = \"la integral definida es\"\n","encoded_seed = torch.tensor([codificar(seed_text)], dtype=torch.long, device=dispositivo)\n","contexto = encoded_seed\n","generated_text = m.generar(contexto, máx_nuevos_tokens=100)\n","print(decodificar(generated_text[0].tolist()))"],"metadata":{"id":"ZyV4_xjfFuB0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715125900634,"user_tz":360,"elapsed":1350,"user":{"displayName":"AI Student","userId":"00754387130632506558"}},"outputId":"d5926606-1e48-4586-830b-12c7b87ed199"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["la integral definida es de cero el siempre para trás de eso es que ya se resulta que está tiene que ser cero esto nuevament\n"]}]},{"cell_type":"code","source":["#Do inference with seed text\n","seed_text = \"el teorema fundamental del calculo nos dice que la suma\"\n","encoded_seed = torch.tensor([codificar(seed_text)], dtype=torch.long, device=dispositivo)\n","contexto = encoded_seed\n","generated_text = m.generar(contexto, máx_nuevos_tokens=500)\n","print(decodificar(generated_text[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cULnSbnsYvwy","executionInfo":{"status":"ok","timestamp":1715124572658,"user_tz":360,"elapsed":4786,"user":{"displayName":"AI Student","userId":"00754387130632506558"}},"outputId":"283ef25c-8191-4140-db64-db8bf0d073e6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["el teorema fundamental del calculo nos dice que la suma relación comúnmutir trite hay agrega lo que necesitamos es precisa a esistro carga 3 si es tomento como desplazar al resencia con esta canercia que vean que sino que está precisamente el 0 solenar la integral de la base tener que está decir por la ap q naría utilizar como conocemos la 1 entonces habica es importa efe a que el producer una vez que la tierce que tubola alfa como lo que voy a darlo tomando está vale conveniente en coeficientes campo vimos a necesitamos habría sino que algo punto a\n"]}]},{"cell_type":"code","source":["#Do inference with seed text\n","seed_text = \" \"\n","encoded_seed = torch.tensor([codificar(seed_text)], dtype=torch.long, device=dispositivo)\n","contexto = encoded_seed\n","generated_text = m.generar(contexto, máx_nuevos_tokens=100)\n","print(decodificar(generated_text[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"szeCPDH4ZCWO","executionInfo":{"status":"ok","timestamp":1715126039594,"user_tz":360,"elapsed":1046,"user":{"displayName":"AI Student","userId":"00754387130632506558"}},"outputId":"04bb9d81-d4ee-44ac-f91c-bf87bfbe8cd6"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":[" la ecuaciente la fuerza de resultado es minúscula para resolver exactivos y ítedo como de resultado \n"]}]},{"cell_type":"code","source":["#Do inference with seed text\n","seed_text = \"por constricciones del problema\"\n","encoded_seed = torch.tensor([codificar(seed_text)], dtype=torch.long, device=dispositivo)\n","contexto = encoded_seed\n","generated_text = m.generar(contexto, máx_nuevos_tokens=150)\n","print(decodificar(generated_text[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dh-yz8aHZIWu","executionInfo":{"status":"ok","timestamp":1715126289947,"user_tz":360,"elapsed":1782,"user":{"displayName":"AI Student","userId":"00754387130632506558"}},"outputId":"076e566d-d202-4432-b768-93e159f4f965"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["por constricciones del problema eso hay algunos aceptos viemos que si no sea un cep son las condinas que una manera la forma de x de x no es vamos a componentes ya la plaza unir era\n"]}]},{"cell_type":"code","source":["#Do inference with seed text\n","seed_text = \"las coordenadas\"\n","encoded_seed = torch.tensor([codificar(seed_text)], dtype=torch.long, device=dispositivo)\n","contexto = encoded_seed\n","generated_text = m.generar(contexto, máx_nuevos_tokens=200)\n","print(decodificar(generated_text[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_AvJQlEfxy0","executionInfo":{"status":"ok","timestamp":1715126514828,"user_tz":360,"elapsed":2727,"user":{"displayName":"AI Student","userId":"00754387130632506558"}},"outputId":"7fa38384-5ecb-4722-bb3f-da8ea558dbc3"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["las coordenadas de volumen simplemente están adelando es terminadas perimas no es el potencias por lo que al invera en campos podicada que en un 3 esta la integral sobre la otra forma es que evaluadas es que fronter\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"J8Z7k5s8gTma"},"execution_count":null,"outputs":[]}]}